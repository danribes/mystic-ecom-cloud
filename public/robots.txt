# robots.txt for Spirituality Platform
# This file tells search engine crawlers which pages or files they can or can't request from your site.
# Learn more: https://developers.google.com/search/docs/advanced/robots/intro

# Allow all crawlers to access all content by default
User-agent: *
Allow: /

# Disallow sensitive and internal paths
Disallow: /api/
Disallow: /admin/
Disallow: /cart/
Disallow: /checkout/
Disallow: /account/orders/
Disallow: /account/settings/
Disallow: /_astro/

# Disallow search result pages with query parameters (to avoid duplicate content)
Disallow: /*?*search=
Disallow: /*?*utm_

# Disallow authentication pages
Disallow: /login
Disallow: /register
Disallow: /forgot
Disallow: /verify-email

# Allow specific public pages even if they're in protected directories
Allow: /api/health
Allow: /api/status

# Sitemap location
Sitemap: https://mystic-international.com/sitemap.xml

# Crawl-delay for specific bots (optional - prevents aggressive crawling)
# Uncomment if needed:
# User-agent: Baiduspider
# Crawl-delay: 5

# Block specific bots (optional - uncomment if needed)
# User-agent: BadBot
# Disallow: /

# Special rules for AI crawlers (optional)
# User-agent: GPTBot
# Disallow: /

# User-agent: ChatGPT-User
# Disallow: /

# User-agent: anthropic-ai
# Allow: /

# User-agent: Claude-Web
# Allow: /

# Additional guidelines:
# - Keep this file updated as your site structure changes
# - Test with Google Search Console's robots.txt Tester
# - Remember that robots.txt is publicly accessible
# - It's a directive, not a firewall - doesn't guarantee security
# - Use noindex meta tags for more control over specific pages
